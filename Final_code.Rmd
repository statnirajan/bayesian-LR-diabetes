---
title: "Bayesian Logistic Regression for Diabetes Prediction"
author: "Nirajan Budhathoki"
date: "9/26/2021"
output: html_document
---
Bayesian methods have been widely used for classification problems in machine learning. This study employs Bayesian technique for predicting diabetes in a well-known dataset.

### The Data 
Data for this study is taken from Kaggle https://www.kaggle.com/uciml/pima-indians-diabetes-database  
The data was collected by National Institute of Diabetes and Digestive and Kidney Diseases. The study participants are all female of PIMA Indian Heritage, at least 21 years old. The data set consists certain diagnostic measurements of 768 females. The variable “Outcome” is the response variable in the data set coded as “0” for absence of diabetes and “1” for the presence. Missing values for predictors were coded as “0” in the dataset. For this study, the missing values for all variables except “Pregnancies” were imputed using median values of the variables.

### Methodology
Binary logistic regression models have been developed to study the classification in two categories of the outcome.
In a logistic regression model with p predictors, the probability of occurrence of an outcome is given as,
 $$
 p_i = P(Y_i =1) = E(Y_i) =  \frac{exp(\beta_0+\beta_1X_1+...+\beta_ 
 pX_p)}{1+exp(\beta_0+\beta_1X_1+...+\beta_
 pX_p)}$$
where i = 1,2,....,n subjects in the study.

Equivalently,
$$
p_i = E(Y_i) = \frac{1}{1+exp(-(\beta_0+\beta_1X_1+...+\beta_
 pX_p))}
$$
Since the response variable $Y_{i}$ takes binary values, we formulate it to follow a Bernoulli distribution. This gives the likelihood function for the model.

$Y_{i}|p_i \sim {\sf Bernoulli}(p_i)$ 

where $p_i$ is just defined in the equation above.

**Prior Distribution on Coefficients:** We take two different piors and compare their performances. First, a non-informative normal prior is taken. This is considered as a default vague prior. Next, we take a Cauchy prior suggested by Gelman et.al.(2008). More on these priors are presented later.

Inferences from Bayesian analysis are summarized in the form of posterior distribution. The posterior distribution updates our prior belief and tells what is known after the data has been observed.


#### Read in the data and do some pre-processing

```{r}
setwd("U:/PRJ/Bayesian")
diab = read.csv("diabetes.csv")
head(diab)

# Value of 0 in the dataset refer missing values. Impute missing values with median for all predictors except "Pregnancies".
myfun = function(x){
  ifelse(x== 0,median(x),x)
}

imp.median <- data.frame(
  sapply(diab[,-c(1,9)],myfun))
finaldata = cbind(diab[,c(9,1)],imp.median)
head(finaldata)
```

Next, we split the data into training and testing sets. Models will be built on training data and their performances will be evaluated on testing data.

```{r}
set.seed(111)
train = sample(1:nrow(finaldata),nrow(finaldata)*0.6)  #60% data on training set
finaldata.train = finaldata[train,]
finaldata.test = finaldata[-train,]
```

Let's see how correlated are the different predictors. High correlation between predictors are problematic if our goal is inference. However, if the goal is prediction, we should not worry much.

```{r message=FALSE, warning=FALSE}
library(corrplot)
Cor = cor(finaldata.train[,-1])
corrplot(Cor, type="upper", method="number", tl.pos="d")
```

Pregnancies and Age seem to have moderate positive correlation. Similar is the story between Skin Thickness and BMI. 

The predictors are measured on various units. Therefore, we are going to scale them so that the posterior results become comparable later on. The Cauchy Prior on regression coefficients requires scaling of predictors to have a mean of 0 and standard deviation 0.5. For consistency, we will use the same scaling while using the default normal prior.

```{r}
X1 = scale(finaldata.train[,-1])*0.5+0
colMeans(X1)
apply(X1,2,sd)
```

The models are developed using JAGS which stands for *Just Another Gibbs Sampler*. JAGS's functionalities can be accessed from R using the "rjags" package. The first step is to specify the model. As discussed preivously, the first prior on intercept as well as other regression coefficients will be a normal prior with mean 0 and variance 100. These values make the prior non-informative and hence the inferences are typically data-driven. JAGS requires model be written as string.

```{r message=FALSE, warning=FALSE}
library(rjags)

mod1_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])         # Likelihood portion
        logit(p[i]) = int + b[1]*Pregnancies[i] + b[2]*Glucose[i] + b[3]*BloodPressure[i] + b[4]*SkinThickness[i] + b[5]*Insulin[i] + b[6]*BMI[i] + b[7]*DiabetesPedigreeFunction[i] + b[8]*Age[i]
    }
    int ~ dnorm(0.0, 1.0/100.0)    # Normal prior with mean 0 and variance 100 (equivalently, precision of 1/100).
    for (j in 1:8) {
        b[j] ~ dnorm(0.0, 1.0/100.0)
    }
} "
```

The second step is to set up the model and tell where the data are in a list.

```{r}
set.seed(33)

data_jags = list(y=finaldata.train$Outcome, Pregnancies=X1[,"Pregnancies"], Glucose=X1[,"Glucose"], BloodPressure=X1[,"BloodPressure"], SkinThickness=X1[,"SkinThickness"], Insulin=X1[,"Insulin"], BMI=X1[,"BMI"], DiabetesPedigreeFunction=X1[,"DiabetesPedigreeFunction"], Age=X1[,"Age"])

# Parameters we may want to monitor are intercept and the other coefficients.
params = c("int", "b")

# Specify the model itself.
mod1 = jags.model(textConnection(mod1_string), data=data_jags, n.chains=3) # Run three different chains with different starting values

# Give a burn-in period of 1000 iterations. Samples are not kept for first 1000 iterations.
update(mod1, 1e3)

# Actual posterior simulations we will keep. Run 5000 iterations.
mod1_sim = coda.samples(model=mod1, variable.names=params, n.iter=5e3) # Simulations are stored as matrices.

# Combine results from 3 different chains into one by stacking matrices that contain simulations.
mod1_csim = as.mcmc(do.call(rbind, mod1_sim))
```

Let's perform some convergence diagnostics for the Markov Chains.

```{r}
# Convergence diagnostics. Start with trace plots.
# plot(mod1_sim, ask=TRUE) # Not displayed in the shared file. Different colors for different chains we ran. Look random (no trend) which is desirable.
gelman.diag(mod1_sim)  # Potential scale reduction factors of 1 indicate models have probably converged.
autocorr.plot(mod1_sim) # Autocorrelation quickly dropped to near zero within first 5 lags. No autocorrelation issue in the estimated coefficients. 
```

Let's also see the model summaries.

```{r}
summary(mod1_sim)
```

#### Prediction

If we have the regression coefficients and the predictor values, we can plug them into the second equation for $p_i$ above to get an estimate of the probability that the Outcome = 1.

```{r}
# Extract posterior mean of coefficients
pm_coef = colMeans(mod1_csim)

# The matrix multiplication below gives the exponentiation part in equation which will then be used to find estimated probabilities.

pm_Xb = pm_coef["int"] + X1[,c(1,2,3,4,5,6,7,8)] %*% pm_coef[1:8] # Intercept + Design Matrix*Coefficients
phat = 1.0 / (1.0 + exp(-pm_Xb))  # Predicted probabilities that the Outcome = 1 for each observations
```

The plot of predicted probabilities against the actual outcome value gives a rough idea on how successful the model is on the training dataset.

```{r}
plot(phat, jitter(finaldata.train$Outcome))
```

Looks okay. Observations with lower probabilities of Outcome=0 assigned by the model were often actually 0 in the dataset. It would be more interesting to see this result in the test dataset.

Let's select 0.5 as the cut-off. Probabilities greater than 0.5 will be labeled '1'(Presence of Diabetes) as the outcome and below 0.5 will be labeled '0' (Absence of Diabetes).

```{r}
(tab0.5 = table(phat > 0.5, finaldata.train$Outcome))
sum(diag(tab0.5)) / sum(tab0.5)  # Correct classification rate in the training dataset
```

Now, let's see the model's performance in the test dataset. Again, we start by standardizing the data.

```{r}
X2 = scale(finaldata.test[,-1])*0.5+0
```

Now, using the coefficients obtained, let's find the predicted probabilities of individual observations in the test dataset.

```{r}
pm_coef = colMeans(mod1_csim)
pm_Xb = pm_coef["int"] + X2[,c(1,2,3,4,5,6,7,8)] %*% pm_coef[1:8] # Intercept + Design Matrix*Coefficients
phat = 1.0 / (1.0 + exp(-pm_Xb))
```

Model performance in the test dataset.
```{r}
plot(phat, jitter(finaldata.test$Outcome))

(tab0.5 = table(phat > 0.5, finaldata.test$Outcome))
sum(diag(tab0.5)) / sum(tab0.5)  # Correct classification rate in the training dataset
```

Similar accuracy as the training data is found. Next, we want to proceed with a different prior and compare the performances of the two models. This time we will use the weakly informative Cauchy priors for the coefficients. As recommended by Gelman et.al (2008), data are first standardized so that all continuous variables have mean 0 and standard deviation 0.5. Then the intercept will be specified a Cauchy prior distribution centered at 0 and scale of 10. The other coefficients will get a scale of 2.5. Again, let's set up, specify and run the model in training data.

```{r}
mod2_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dbern(p[i])
        logit(p[i]) = int + b[1]*Pregnancies[i] + b[2]*Glucose[i] + b[3]*BloodPressure[i] + b[4]*SkinThickness[i] + b[5]*Insulin[i] + b[6]*BMI[i] + b[7]*DiabetesPedigreeFunction[i] + b[8]*Age[i]
    }
    int ~ dt(0, 1/10^2, 1)    # t prior with mean 0 and scale 10.This is weakly informative chaucy prior.
    for (j in 1:8) {
        b[j] ~ dt(0, 1/2.5^2, 1)  #  t prior with mean 0 and scale 2.5
    }
}"
```

Specify the parameters and run MCMC.

```{r}
set.seed(44)

data_jags = list(y=finaldata.train$Outcome, Pregnancies=X1[,"Pregnancies"], Glucose=X1[,"Glucose"], BloodPressure=X1[,"BloodPressure"], SkinThickness=X1[,"SkinThickness"], Insulin=X1[,"Insulin"], BMI=X1[,"BMI"], DiabetesPedigreeFunction=X1[,"DiabetesPedigreeFunction"], Age=X1[,"Age"])

# Parameters we may want to monitor are intercept and the other coefficients.
params = c("int", "b")

# Specify the model itself.
mod2 = jags.model(textConnection(mod2_string), data=data_jags, n.chains=3) # Run three different chains with different starting values

# Give a burn-in period of 1000 iterations. Samples are not kept for first 1000 iterations.
update(mod2, 1e3)

# Actual posterior simulations we will keep. Run 5000 iterations.
mod2_sim = coda.samples(model=mod2, variable.names=params, n.iter=5e3) # Simulations are stored as matrices.

# Combine results from 3 different chains into one by stacking matrices that contain simulations.
mod2_csim = as.mcmc(do.call(rbind, mod2_sim))
```

Again, let's perform some convergence diagnostics.
```{r}
# Convergence diagnostics. Start with trace plots.
# plot(mod2_sim, ask=TRUE) # Not displayed in the shared file. Different colors for different chains we ran. Look random (no trend) which is desirable.
gelman.diag(mod2_sim)  # Potential scale reduction factors of 1 indicate models have probably converged.
autocorr.plot(mod2_sim) # Autocorrelation quickly dropped to near zero within first 5 lags. No autocorrelation issue in the estimated coefficients.
```


Model summary again.

```{r}
summary(mod2_sim)
```

#### Prediction

```{r}
# Extract posterior mean of coefficients
pm_coef2 = colMeans(mod2_csim)

# The matrix multiplication below gives the exponentiation part in equation which will then be used to find estimated probabilities.

pm_Xb2 = pm_coef2["int"] + X1[,c(1,2,3,4,5,6,7,8)] %*% pm_coef2[1:8] # Intercept + Design Matrix*Coefficients
phat2 = 1.0 / (1.0 + exp(-pm_Xb2))  # Predicted probabilities that the Outcome = 1 for each observations
```

```{r}
plot(phat2, jitter(finaldata.train$Outcome))
```
```{r}
(tab0.5 = table(phat2 > 0.5, finaldata.train$Outcome))
sum(diag(tab0.5)) / sum(tab0.5)  # Correct classification rate in the training dataset
```

Now, let's see the performance in test dataset.

```{r}
pm_coef2 = colMeans(mod2_csim)
pm_Xb2 = pm_coef2["int"] + X2[,c(1,2,3,4,5,6,7,8)] %*% pm_coef2[1:8] # Intercept + Design Matrix*Coefficients
phat2 = 1.0 / (1.0 + exp(-pm_Xb2))
```


```{r}
plot(phat2, jitter(finaldata.test$Outcome))
```

```{r}
(tab0.5 = table(phat2 > 0.5, finaldata.test$Outcome))
sum(diag(tab0.5)) / sum(tab0.5)  # Correct classification rate in the testing dataset
```

Both priors lead to almost exact result. Therefore, in these cases the posterior analysis is largely driven by the likelihood and not the prior specification. In general, when n >>p, we don't expect much difference in results from different priors. With both priors, we achieved slightly higher than 77% classification accuracy in the test dataset.


### References and Links
[1] Andrew Gelman. Aleks Jakulin. Maria Grazia Pittau. Yu-Sung Su. "A weakly informative default prior distribution for logistic and other regression models." Ann. Appl. Stat. 2 (4) 1360 - 1383, December 2008.

[2] Matthew Heiner.Bayesian Statistics: Techniques and Models.Coursera. https://www.coursera.org/learn/mcmc-bayesian-statistics






